\chapter{Knowledge graphs}\label{kgs}
In this chapter, we present the theoretical background and applications of knowledge graphs. We begin by defining what knowledge graphs are in Section~\ref{definition}, followed by the applications of these graphs in biomedicine in Section~\ref{kgs:biomed}.

\section{Definition}\label{definition}
The concept of knowledge graphs has been a topic of research particularly within the Semantic Web community since Google's 2012 announcement of its Knowledge Graph. Despite their extensive use, a unified definition remains elusive. Various definitions from literature show significant variation in scope and focus. Below, we present a selection of them to illustrate this diversity:
\begin{description}
    \item[Structure and Composition] Paulheim~\cite{Paulheim2016KnowledgeGR} proposed a minimal set of characteristics that distinguish KGs as graph-based knowledge representations: ``A knowledge graph (i) mainly describes real world entities and their interrelations, organized in a graph, (ii) defines possible classes and relations of entities in a schema, (iii) allows for potentially interrelating arbitrary entities with each other and (iv) covers various topical domains''. Similarly, the Journal of Web Semantics describes KGs as ``graphs of entities, their types, and relationships between entities''~\cite{Kroetsch2016knowledge}
    \item[Domain-Specific Networks] The Semantic Web Company highlights the flexibility of KGs to represent domain-specific networks, incorporating not just concepts but also instances like documents and datasets.~\cite{Blumauer2014knowledge}
    \item[Formal Definition] As proposed by FÃ¤rber et al. in~\cite{Farber2016linked} a knowledge graph can be defined as an RDF graph. An RDF graph consists of a set of triples $(s, p, o)$ is an ordered set of the following RDF terms: 
    \begin{itemize}
        \item a subject $s\in U \cup B$
        \item an object $o\in U\cup B\cup L$
        \item a predicate $p\in U$
    \end{itemize}
    An RDF term is either a URI $u\in U$, a blank node $b\in B$, or a literal $l\in L$.
    \item[Dynamic Extraction] Pujara et al.~\cite{Pujara2013KGIdentification} focus on the automatic extraction of knowledge from the web in the form of facts that are interrelated.
\end{description}

This diversity in definitions hinders precise discussions and comparative research, emphasizing the need for a common, comprehensive definition to advance the field. To address this gap, the authors in~\cite{Ehrlinger2016TowardsAD} propose a new definition of a knowledge graph: 
\begin{center}
    \begin{quote}
        \term{A knowledge graph acquires and integrates information into an ontology and applies a reasoner to derive new knowledge.}
    \end{quote}
\end{center}
This definition seeks to unify the understanding of KGs by emphasizing three critical features:
\begin{description}
    \item[Information integration] KGs collect, extract, and integrate information from multiple external sources, making them dynamic and scalable.
    \item[Reasoning capabilities] The inclusion of a reasoning engine allows KGs to infer new knowledge from existing data, which elevates their functionality beyond a static repository.
    \item[Ontological foundation] KGs are built on an ontological foundation, which provides a formal structure for representing knowledge and relationships between entities. 
\end{description}

The authors stress that a KG is more complex than a traditional knowledge base or ontology, as it combines these features to support automated reasoning and dynamic knowledge integration. The size of the KG is considered less critical compared to its functional capabilities.

\section{Data graphs}\label{data-graphs}
The foundation of any knowledge graph lies in abstracting data into a graph structure, resulting in an initial \term{data graph}. In the following sections we will refer to them as, where Modelling data in this way offers more flexibility for integrating new sources of data compared to the standard relational model, where a schema must be defined upfront and followed at each step. While other structured data models such as trees offer similar flexibility, graphs do not require organizing the data hierarchically and allow cycles to be represented and queried. Here, we explore a range of graph-structured data models frequently used to represent such graphs in practice.~\cite{Hogan2021KGs}. 

For the concepts in the following sections we provide the formal definitions in Appendix~\ref{app:data-graphs}.

\subsection{Models}\label{models}
We begin by describing the main graph structures used to represent knowledge graphs. In the following paragraphs, we define progressively more complex models. 

\subsubsection{Directed edge-labelled graphs}
A directed edge-labelled graph is defined as a set of nodes and a set of directed labelled edges between these nodes. In KGs, the nodes represent entities and the edges represent relationships between these entities. A standardized data model based on directed edge-labelled graphs is the Resource Description Framework (RDF)~\cite{Cyganiak2014rdf}, which has been recommended by the W3C. The RDF model defines different types of nodes, including Internationalized Resource Identifiers (IRIs) [134], which allow for global identification of entities on the Web; literals, which allow for representing strings (with or without language tags) and other datatype values (integers, dates, etc.); and blank nodes, which are anonymous nodes that are not assigned an identifier.

\subsubsection{Heterogeneous graphs}
Heterogeneous graphs are a generalization of directed edge-labelled graphs that allow nodes and edges to be of different \term{types}. An edge can be \term{homogeneous} if it connects two nodes of the same type, or \text{Heterogeneous} if it is between nodes of different types. This labelling allows to partition the graph according to the node types, which can be useful for machine learning tasks.

\subsubsection{Property graphs}
The structure provide additional flexibility when modelling data as a graph. It adds to heterogeneous graphs a set of \text{property-value} pairs and it allows to associate a \term{label} to both nodes and edges.

It is always possible to translate a property graph from and into a directed edge-labelled graph without information loss. Though the models are equivalent, directed edge-labelled models offer a more minimal model, while property graphs are more flexible. Often the choice boils down to practical factors, such as the available implementations of the various models.

\subsubsection{Other graph data models}
One can extend the structures shown above, for example by introducing \term{complex nodes}  or \term{hypernodes} that could contain individual edges or nested graphs. Conversely \term{hypergraphs} define \term{complex edges} that may connect more than two nodes.

\subsection{Querying}\label{querying}
A number of practical languages have been proposed for querying graphs~\cite{Angles2017FoundationmodernQueryLnguagesforGraphDatabases}, including the SPARQL query language for RDF graphs~\cite{Harris2013SPARQL}; and Cypher~\cite{Francis2018Cypher}, Gremlin~\cite{Rodriguez2015Gremlin}, and G-CORE~\cite{Angles2018G-CORE} for querying property graphs. Underlying these query languages are some common primitives, including graph patterns, relational operators, path expressions, and more besides~\cite{Angles2017FoundationmodernQueryLnguagesforGraphDatabases}. We now describe these core features for querying graphs in turn, starting with graph patterns. 

\subsubsection{Graph patterns}
At the basis of every structured query language of graphs are \term{(basic) graph patterns}, that are related with the graph model that is being queried. In directed edge-labelled graphs the terms are its nodes and edge labels. The terms in property graphs are its ids, labels, properties and values.

Terms are divided into constants and variables. The graph pattern is evaluated against the data graph by generating mappings from the variables of the graph pattern to constants in the data graph, such that the image of the graph pattern obtained by assigning constants to the variables is contained within the graph. 


The variables in the graph patterns could be mapped to the same constant terms, but this may not be desirable for all applications. So, various semantics have been proposed to define the evaluation of graph patterns. The most important are:
\begin{itemize}
    \item\term{homomorphism-based semantics} allows multiple variables to be mapped to the same term.
    \item\term{isomorphic-based semantics} requires nodes and edge variables to be mapped to distinct terms. 
\end{itemize}
Different practical languages adopt different semantics: SPARQL is based on homomorphism-based semantics, whilst Cypher has chosen for isomorphic-based semantics on edges.

\subsubsection{Complex graph patterns}
Given an input graph, a graph pattern transforms it into a table of results. These can be considered using relational algebra to form more complex queries on the graph. Recall that relational algebra consists of various operations:
\begin{itemize}
    \item Unary operations:
    \begin{itemize}
        \item \term{projection ($\pi$)} to output a subset of columns,
        \item \term{selection ($\sigma$)} to output a subset of rows,
        \item \term{column renaming ($\rho$)}.
    \end{itemize}
    \item Binary operations:
    \begin{itemize}
        \item \term{union ($\cup$)} to merge rows of two tables,
        \item \term{difference (-)} to remove rows from one table that are present in another,
        \item \term{joins ($\bowtie$)} to combine rows from two tables based on a related column.
    \end{itemize}
\end{itemize}
Graph patterns can be expressed in a subset of relational algebra formed by $\pi$, $\sigma$, $rho$ and $\bowtie$. Specifically only variables not fixed to a constant can be projected in a graph pattern.

\subsubsection{Navigational graph patterns}
The key difference between graph guery languages id the ability to include \term{path expressions} in queries. A path expression $r$ is a regular expression that allows matching arbitrary length paths between nodes, which is expressed as a \text{regular path query} $(x,r,y)$, where $x$ and $y$ are terms (even the same term). Regular path queries are defined recursively as follows: the base path expression is where $r$ is a constant edge label. If $r$ is a path expression, then also its inverse $r^-$ and Kleene star $r^*$ are also path expressions. Furthermore if $r_1$ and $r_2$ are path expressions, then also their disjuntion $r_1mid r_2$ and their concatenation $r_1\cdot r_2$ are path expressions.

There are various semantics to evaluate regular path queries to account for graphs containing loops, that when evaluated could return infinite matching regular path expressions. For example one can return only the shortest paths or paths without repeated nodes and edges (as done in Cypher). Another option is to return the finite set of pairs of nodes connected by a matching path instead of specifying the entire path.

\section{Schema, identity, context}\label{schema-identity-context}
In this section we describe various enhancements and extensions of the data graph â relating to schema, identity and context â that provide additional structures for accumulating knowledge. Henceforth, we refer to a \term{data graph} as a collection of data represented as nodes and edges using one of the models discussed in~\ref{data-graphs}. We refer to a \term{knowledge graph} as a data graph potentially enhanced with representations of schema, identity, context, ontologies and/or rules. These additional representations may be embedded in the data graph, or layered above it.

\subsection{Schema}\label{schema}
One of the benefits of modelling data as graphs â versus, for example, the relational model â is the option to forgo or postpone the definition of a schema. However, when modelling data as graphs, schemata can be used to prescribe a high-level structure and/or semantics that the graph follows or should follow. We discuss three types of graph schemata: \term{semantic}, \term{validating}, and \term{emergent}.

\subsubsection{Semantic schema}\label{schema_semantic}
A \term{semantic schema} allows for defining the meaning of high-level terms (aka \term{vocabulary} or \term{terminology}) used in the graph, which facilitates reasoning over graphs using those terms. We may notice some natural groupings of nodes based on the types of entities to which they refer. We may thus decide to define \term{classes} to denote these groupings. Additionally, we may observe some natural relations between some of these classes that we would like to capture. A class hierarchy can be established where children are defined to be \term{subclasses} of their parents. This allows for inferring higher-level class memberships from lower-level ones.

Aside from classes, we may also wish to define the semantics of edge labels, also known as \term{properties}. Properties can be organized into hierarchies, where specific properties are \term{sub-properties} of more general ones. We can further define the \term{domain} and \term{range} of properties, indicating the class(es) of entities for nodes from which and to which edges with that property extend, respectively. This allows for inferring class memberships based on property relations.

A prominent standard for defining a semantic schema for (RDF) graphs is the \textit{RDF Schema} (RDFS) standard~\cite{Brickley2014RDFSchema1.1}, which allows for defining subclasses, subproperties, domains, and ranges amongst the classes and properties used in an RDF graph, where such definitions can be serialised as a graph. We illustrate the semantics of these features in Table~\ref{tab:semanticSchemaFeatures}. These definitions can then be embedded into a data graph. More generally, the semantics of terms used in a graph can be defined in much more depth than seen here, as is supported by the Web Ontology Language (OWL) standard~\cite{Hitzler2014OWLPrimer} for RDF graphs.

Semantic schema are typically defined for incomplete graph data, here the absence of an edge between two nodes does not mean that the relation does not hold in the real world. Therefore one can not assume that the graph is a complete description of the world. These systems are said to adopt the \term{Open World Assumption} (\term{OWA}), in opposition to the \term{Closed World Assumption} (\term{CWA}), that allows to state with certainty if a relation exists in the world. A consequence of CWA is that the addition of an edge to the data graph may contradict what was previously assumed to be false (due to missing information), whereas with OWA, a statement that is proven false continues to be false with the addition of more edges.

\input{figs/tabSemanticSchemaFeatures}

\subsubsection{Validating schema}
When graphs are used to represent diverse, incomplete data at large-scale, the OWA is the most appropriate choice for a default semantics. But in some scenarios, we may wish to guarantee that our data graph â or specific parts thereof â are in some sense âcompleteâ. We can define such constraints in a \term{validating schema} and validate the data graph with respect to the resulting schema, listing constraint violations (if any). Thus, while semantic schemata allow for inferring new graph data, validating schemata allow for validating existing graph data. 

A standard way to define a validating schema for graphs is using \term{shapes}~\cite{Knublauch2017SHACL}\cite{LabraGayo2017ValidatingRDF}\cite{Prudhommeaux2014ShapeExpressions}. A shape \term{targets} a set of nodes in a data graph and specifies \term{constraints} on those nodes. The shapeâs target can be defined in many ways, such as targeting all instances of a class, the domain or range of a property, the result of a query, nodes connected to the target of another shape by a given property, etc. Constraints can then be defined on the targeted nodes, such as to restrict the number or types of values taken on a given property. When declaring shapes, the data modeller may not know in advance the entire set of properties that some nodes can have. An \term{open shape} allows the node to have additional properties not specified by the shape, while a \term{closed shape} does not.

Although validating schemata and semantic schemata serve different purposes, they can complement each other. In particular, a validating schema can take into consideration a semantic schema, such that, for example, validation is applied on the data graph including inferences. The presence of a semantic schema may, however, require adapting the validating schema. Open shapes may also be preferred in such cases rather than enumerating constraints on all possible properties that may be inferred on a node.

\subsubsection{Emergent schema}
Both semantic and validating schemata require a domain expert to explicitly specify definitions and constraints. However, a data graph will often exhibit latent structures that can be automatically extracted as an \term{emergent schema}~\cite{Pham2015EmergenSchemaFromRDF}, or a \term{graph summary}~\cite{Cebiric2019SummarizingSemanticGraphs}\cite{Liu2018GraphuSummarizationMethodsAndApplications}\cite{Spahiu2016ABSTAT}. A framework often used for defining emergent schema is that of quotient graphs, which partition groups of nodes in the data graph according to some equivalence relation while preserving some
structural properties of the graph. Merging the nodes of each partition into one node while preserving edges leads to the quotient graph. Specifically, each of the nodes in the quotient graph is a partition of the original nodes in the data graph, while an edge $(X,Y)$ is in the quotient graph if and only if there exist $x\in X$ and $y\in Y$ such that $(x,y)$ is an edge in the data graph. 

There are many ways in which quotient graphs may be defined, resulting in different guarantees with respect to the structure they preserve. Regardless, a quotient graph must \term{simulate} its input graph, meaning that for all input nodes $x$ and quotient nodes $X$ such that $x\in X$ , if $(x,y)$ is an edge in the data graph, then there must exist an edge $(X,Y)$ in the quotient graph such that $y\in Y$. A stronger notion of structural preservation is given by \term{bisimilarity}, which requires that if $(X,Y)$ is an edge in the quotient graph, then for all $x\in X$, there must exist a $y\in Y$ such that $(x,y)$ is in the data graph. We provide formal definitions for the notions of quotient graphs, simulation and bisimulation in Appendix~\ref{app:emergent-schemata}.

There are many ways in which quotient graphs may be defined, depending on the equivalence relation that partitions nodes. Furthermore, there are many ways in which other similar or bisimilar graphs can be defined, depending on the (bi)simulation relation that preserves the data graphâs structure~\cite{Cebiric2019SummarizingSemanticGraphs}. Such techniques aim to summarise the data graph into a higher-level topology. In order to reduce the memory overhead of the quotient graph, in practice, nodes may rather be labelled with the cardinality of the partition and/or a high-level label for the partition rather than storing the labels of all nodes in the partition.

Various other forms of emergent schema not based on a quotient graph framework have also been proposed; examples include emergent schemata based on relational tables~\cite{Pham2015EmergenSchemaFromRDF}, formal concept analysis~\cite{Gonzalez2018ModellingDynamicsSemanticWebKGFormalConceptAnalysis}, and so forth. Emergent schemata may be used to provide a human-understandable overview of the data graph, to aid with the definition of a semantic or validating schema, to optimise the indexing and querying of the graph, to guide the integration of data graphs, and so forth, as further explored in~\cite{Cebiric2019SummarizingSemanticGraphs}.

\subsection{Identity}\label{identity}
To avoid possible label ambiguity, we may use globally-unique identifiers to avoid naming clashes when the knowledge graph is extended with external data, and we may add external identity links to disambiguate a node with respect to an external source.

When merging knowledge graphs, ambiguous nodes can lead to naming clashes, so \term{Persistent Identifiers} (PIDs) are used to uniquely identify entities, like DOI for papers, ORCID for authors, ISBN for books. RDF extends this by recommending Internationalized Resource Identifiers (IRIs) instead of simple URLs to distinguish between web pages and real-world entities (e.g., Wikidata IRIs). This ensures clarity and avoids misleading relationships between entities and their descriptive resources \cite{Hakala2010PIDs}.

While HTTP IRIs provide global identifiers, they are not inherently persistent websites may go offline or URLs may change. \term{Persistent URL} (PURL) services enhance persistence by redirecting a stable identifier to the current location of a resource. This mechanism ensures that references remain valid even if the actual resource moves, maintaining consistency across RDF graphs. PURL services also support namespaces, allowing RDF data to remain reliable over time~\cite{BernersLee2006LinkedData}\cite{Heath2011LinkedData}.

Even with IRIs, different knowledge graphs may use distinct identifiers for the same entity. To address this, identity links help unify different references. The OWL standard includes \texttt{owl:sameAs} to establish equivalence between entities from different sources, allowing merged graphs to consolidate data correctly.

Identifiers themselves do not always carry semantic meaning. For example, Wikidata uses numerical IRIs to maintain language neutrality and persistence. To enhance human readability, RDF graphs often include labels, aliases, and comments. Multilingual lexicalized knowledge graphs further support entity recognition across different languages and text corpora, improving usability and searchability~\cite{DeMelo2015Lexvo.org}\cite{MartinezRodriguez2020InformationExtractionMeetsSemanticWeb}.

Graphs sometimes need to represent unknown entities, such as an unspecified event venue. Instead of omitting information, existential nodes (or blank nodes in RDF) allow graphs to indicate the existence of an entity without naming it. These are useful in structuring data, such as ordered RDF lists, but can complicate operations like graph matching. To mitigate this, techniques like skolemisation assign canonical labels to blank nodes, while some researchers suggest minimizing their use for better graph manageability~\cite{Cyganiak2014rdf}\cite{Hogan2017CanonicalFormsIsomorphicEquivalentRDFGraphs}\cite{Longley2019RDFDatasetNormalization}.

\section{Deductive knowledge}\label{deductive-knowledge}
As humans, we can deduce more from the data graph than what the edges explicitly indicate. In these cases, given the data as premises, and some general rules about the world that we may know a priori, we can use a deductive process to derive new data, allowing us to know more than what is explicitly given by the data. These types of general premises and rules, when shared by many people, form part of ``commonsense knowledge''~\cite{McCarthy1990FomalizingCommonsense}; conversely, when shared by a few experts in an area, they form part of ``domain knowledge''. 

Machines, in contrast, do not have a priori access to such deductive faculties; rather they need to be given formal instructions, in terms of premises and entailment regimes, in order to make similar deductions to what a human can make. These entailment regimes normalise the conclusions that logically follow as a consequence of a given set of premises. Once instructed in this manner, machines can (often) apply deductions with a precision, efficiency, and scale beyond human performance. These deductions may serve a range of applications, such as improving query answering, (deductive) classification, finding inconsistencies, etc. 

Although we have already discussed in Section~\ref{schema_semantic} how semantic schemata can capture subclass relations, we now discuss how more complex entailments can be captured and automated. Though one could leverage logical frameworks like First-Order Logic, we focus on ontologies which constitute a formal representation of knowledge that, importantly for us, can be represented as a graph. We then discuss how these ontologies can be formally defined, how they relate to existing logical frameworks, and how reasoning can be conducted with respect to such ontologies.

\subsection{Ontologies}\label{ontologies}
To enable entailment, we must be precise about what the terms we use mean. Since there is no ``correct'' meaning for a given term we may, its definition is a matter of convention.

The term ontology stems from philosophical study of ontology, concerned with the different kinds of entities that exist, the nature of their existence, what kinds of properties they have, and how they may be identified and categorised. In the context of computing, an \term{ontology} is then a concrete, formal representation of what terms mean within the scope in which they are used (e.g., a given domain). 

Like all conventions, the usefulness of an ontology depends on the level of agreement on what that ontology defines, how detailed it is, and how broadly and consistently it is adopted. Adoption of an ontology by the parties involved in one knowledge graph may lead to a consistent use of
terms and consistent modelling in that knowledge graph. Agreement over multiple knowledge graphs will, in turn, enhance the interoperability of those knowledge graphs.

Amongst the most popular ontology languages used in practice are the \textit{Web Ontology Language}
(\textit{OWL})~\cite{Hitzler2014OWLPrimer}, recommended by the W3C and compatible with RDF graphs; and the \textit{Open Biomedical
Ontologies Format} (\textit{OBOF})~\cite{Mungall2012OBOF}, used mostly in the biomedical domain. 

\subsubsection{Interpretation}
As humans we may interpret a node or an edges in the data graph as referring, respectively, to the real-world entity or relation. We thus interpret the data graph as another graph composed of real-world entities and relations, called a \term{domain graph}. The process of interpretation consists in mapping the nodes and the edges in the data graph to entities and relations in the domain graph. So, we can abstractly define an \term{interpretation} of a data graph as being composed of a domain graph ad a mapping from the \term{terms} (nodes and edge labels) of the data graph to those of the domain graph. Consequently the domain graph follows the same model as the data graph. Such abstract notion of interpretation is useful to define the meaning of ontology features and entailment. Under the Open World Assumption (OWA) (recall from Section~\ref{schema_semantic}), if an edge does not exist between two nodes in the data graph, similarly it cannot exist in the domain graph, since what is not known is assumed to be false.  Conversely, under the Open World Assumption (OWA), we cannot be certain that this relation does not exist as this could be part of some knowledge not (yet) described by the graph. 

These assumptions (or lack thereof) define which interpretations are valid, and which interpretations \term{satisfy }which data graphs. Other assumptions regar the uniqueness of terms, specifically under the \term{Unique Name Assumption} (\term{UNA}) interpretations theat map two data terms to the same domain term are forbidden. Vice versa, the \term{NUNA} (\term{No Unique Name Assumption}) allows such interpretations. 

In addition to ones base assumptions, one can associate certain patterns in the data graph with \term{semantic conditions} that define which interpretations satisfy it. For example, if a data graph contains a relation that states that $p$ is a subclass of $q$ one could enforce that if the domain graph there is the edge $(x,y,p)$, also the edge $(x,y,q)$ must exist.



\section{Applications in biomedicine}\label{biomed}
% The result is most often used in application scenarios that involve integrating, managing and extracting value from diverse sources of data at large scale [387]. Employing a graph-based abstraction of knowledge has numerous benefits in such settings when compared with, for example, a relational model or NoSQL alternatives. Graphs allow maintainers to postpone the definition of a schema, allowing the data â and its scope â to evolve in a more flexible manner than typically possible in a relational setting, particularly for capturing incomplete knowledge [3]. Unlike (other) NoSQL models, specialised graph query languages support not only standard relational operators (joins, unions, projections, etc.), but also navigational operators for recursively finding entities connected through arbitrary-length paths [16]. Standard knowledge representation formalisms â such as ontologies [70, 239, 366] and rules [254, 288] â can be employed to define and reason about the semantics of the terms used to label and describe the nodes and edges in the graph. Scalable frameworks for graph analytics [335, 503, 563] can be leveraged for computing centrality, clustering, summarisation, etc., in order to gain insights about the domain being described. Various representations have also been developed that support applying machine learning techniques directly over graphs [549, 559].~\cite{Hogan2021KGs}
